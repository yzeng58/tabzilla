{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this script, make sure that the metafeatures and metadataset files exist in the TabZilla directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from analysis_utils import get_tuned_alg_perf\n",
    "import pdb, os, sys\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "metadata_folder = Path(\"../TabZilla\")\n",
    "\n",
    "metadataset_df = pd.read_csv(metadata_folder / \"tutorials/metadataset_new.csv\")\n",
    "\n",
    "# keep only binary and classification datasets. we have some results for regression datasets, which are not used.\n",
    "metadataset_df = metadataset_df.loc[metadataset_df[\"target_type\"].isin([\"binary\", \"classification\"]), :]\n",
    "\n",
    "# read metafeatures\n",
    "metafeatures_df = pd.read_csv(Path(\"../TabZilla/metafeatures.csv\"))\n",
    "\n",
    "# get the number of instances for each dataset, we will use these later\n",
    "num_instances = metafeatures_df.loc[:, [\"dataset_name\", \"f__pymfe.general.nr_inst\"]]\n",
    "num_instances.columns = [\"dataset_fold_id\", \"num_inst\"]\n",
    "\n",
    "# make sure that the cleaned_results folder exists\n",
    "output_folder = Path(\"./cleaned_results\")\n",
    "output_folder.mkdir(exist_ok=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in num instances - for runtime calculation\n",
    "metadataset_df = metadataset_df.merge(num_instances, on=\"dataset_fold_id\", how=\"left\")\n",
    "\n",
    "# calculate runtime\n",
    "time_col = \"training_time\"\n",
    "\n",
    "time_per_inst_col = \"train_per_1000_inst\"\n",
    "\n",
    "metadataset_df.loc[:, time_per_inst_col] = 1000. * metadataset_df[time_col] / metadataset_df[\"num_inst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(table_idx):\n",
    "    if table_idx == 1:\n",
    "        from analysis.table1 import DATASETS\n",
    "    elif table_idx == 2:\n",
    "        from analysis.table2 import DATASETS\n",
    "    elif table_idx == 4:\n",
    "        from analysis.table4 import DATASETS\n",
    "        \n",
    "    return DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadataset_df = metadataset_df[metadataset_df.dataset_name.isin(get_datasets(table_idx))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print number of results per dataset and alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for each alg: number of datasets with results (out of 176)\n",
      "alg_name\n",
      "TabPFNModel            63\n",
      "NAM                    80\n",
      "DeepFM                 90\n",
      "Ours                  115\n",
      "TabTransformer        124\n",
      "SAINT                 138\n",
      "NODE                  141\n",
      "SVM                   143\n",
      "DANet                 147\n",
      "rtdl_FTTransformer    148\n",
      "VIME                  163\n",
      "STG                   164\n",
      "LightGBM              165\n",
      "CatBoost              165\n",
      "KNN                   167\n",
      "LinearModel           168\n",
      "TabNet                168\n",
      "RandomForest          173\n",
      "XGBoost               174\n",
      "rtdl_ResNet           174\n",
      "MLP                   175\n",
      "DecisionTree          175\n",
      "rtdl_MLP              176\n",
      "Name: dataset_name, dtype: int64\n",
      "for each dataset: number of algs with results (out of 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset_name\n",
       "openml__poker-hand__9890             6\n",
       "openml__covertype__7593              7\n",
       "openml__Devnagari-Script__167121     8\n",
       "openml__albert__189356              10\n",
       "openml__CIFAR_10__167124            10\n",
       "                                    ..\n",
       "openml__cylinder-bands__14954       23\n",
       "openml__credit-approval__29         23\n",
       "openml__socmob__3797                23\n",
       "openml__colic__27                   23\n",
       "openml__kc2__3913                   23\n",
       "Name: alg_name, Length: 176, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each alg, for how many datasets are there results?\n",
    "print(f\"for each alg: number of datasets with results (out of {len(metadataset_df['dataset_name'].unique())})\")\n",
    "print(metadataset_df.groupby(\"alg_name\")[\"dataset_name\"].apply(lambda x: len(set(x))).sort_values())\n",
    "\n",
    "print(f\"for each dataset: number of algs with results (out of {len(metadataset_df['alg_name'].unique())})\")\n",
    "metadataset_df.groupby(\"dataset_name\")[\"alg_name\"].apply(lambda x: len(set(x))).sort_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Dataset inclusion/exclusion\n",
    "\n",
    "**In this notebook: selected-18-algs:**\n",
    "* We use a list of 18 algs (excluding 3 that had lots of errors.)\n",
    "* We take only the datasets where each of these algs produce a result. This is ~100 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for each dataset: number of algs with results\n",
      "dataset_name\n",
      "openml__poker-hand__9890             6\n",
      "openml__covertype__7593              7\n",
      "openml__Devnagari-Script__167121     8\n",
      "openml__albert__189356              10\n",
      "openml__CIFAR_10__167124            10\n",
      "Name: alg_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"for each dataset: number of algs with results\")\n",
    "alg_counts = metadataset_df.groupby(\"dataset_name\")[\"alg_name\"].agg(lambda x: len(set(x))).sort_values()\n",
    "print(alg_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop:\n",
    "# - TabPFN (only runs for small datasets)\n",
    "# - NAM (lots of errors, long runtime)\n",
    "# - DeepFM (not implemented for multi-class)\n",
    "# - TabTransformer (lots of bugs...)\n",
    "\n",
    "# selected_algs = [\n",
    "#     \"SAINT\",\n",
    "#     \"NODE\",\n",
    "#     \"SVM\",\n",
    "#     \"DANet\",\n",
    "#     \"rtdl_FTTransformer\",\n",
    "#     \"VIME\",\n",
    "#     \"STG\",\n",
    "#     \"CatBoost\",\n",
    "#     \"LightGBM\",\n",
    "#     \"KNN\",\n",
    "#     \"LinearModel\",\n",
    "#     \"TabNet\",\n",
    "#     \"RandomForest\",\n",
    "#     \"XGBoost\",\n",
    "#     \"rtdl_ResNet\",\n",
    "#     \"MLP\",\n",
    "#     \"DecisionTree\",\n",
    "#     \"rtdl_MLP\",\n",
    "#     \"NAM\",\n",
    "#     \"TabPFNModel\", # not included in this version\n",
    "#     \"DeepFM\",\n",
    "#     \"TabTransformer\", \n",
    "# ]\n",
    "\n",
    "# test_df = metadataset_df.loc[metadataset_df[\"alg_name\"].isin(selected_algs), :]\n",
    "\n",
    "# # keep only datasets where all selected algs produce a result\n",
    "# alg_count = test_df.groupby(\"dataset_name\")[\"alg_name\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "# keep_datasets = alg_count[alg_count == len(selected_algs)].index\n",
    "\n",
    "# print(f\"keeping {len(keep_datasets)} datasets\")\n",
    "\n",
    "# keep_df = test_df.loc[test_df[\"dataset_name\"].isin(keep_datasets), :]\n",
    "\n",
    "keep_df = metadataset_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly more data prep\n",
    "\n",
    "Note: We will keep all algs, regardless of how many datasets they have results for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing datasets: number of datasets with results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alg_name\n",
      "TabPFNModel            63\n",
      "NAM                    80\n",
      "DeepFM                 90\n",
      "Ours                  115\n",
      "TabTransformer        124\n",
      "SAINT                 138\n",
      "NODE                  141\n",
      "SVM                   143\n",
      "DANet                 147\n",
      "rtdl_FTTransformer    148\n",
      "VIME                  163\n",
      "STG                   164\n",
      "LightGBM              165\n",
      "CatBoost              165\n",
      "KNN                   167\n",
      "LinearModel           168\n",
      "TabNet                168\n",
      "RandomForest          173\n",
      "XGBoost               174\n",
      "rtdl_ResNet           174\n",
      "MLP                   175\n",
      "DecisionTree          175\n",
      "rtdl_MLP              176\n",
      "Name: dataset_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "analysis_df = keep_df\n",
    "\n",
    "print(\"after removing datasets: number of datasets with results\")\n",
    "dataset_counts = analysis_df.groupby(\"alg_name\")[\"dataset_name\"].agg(lambda x: len(set(x))).sort_values()\n",
    "print(dataset_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Tune and rank algorithms for each dataset\n",
    "\n",
    "**Note**: At this point, you should have a dataframe called `analysis_df`, which contains all results you want to include in the remainder of the analysis. \n",
    "\n",
    "The code below performs hyperparameter tuning & ranking of each alg, and writes four cleaned results files to the directory `./cleaned_results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = [\n",
    "    \"Accuracy\",\n",
    "    \"F1\",\n",
    "    \"Log Loss\",\n",
    "    \"AUC\",\n",
    "]\n",
    "\n",
    "obj_type_list = [\n",
    "    \"maximize\",\n",
    "    \"maximize\",\n",
    "    \"minimize\",\n",
    "    \"maximize\",\n",
    "]\n",
    "result_df_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace alg name with display name\n",
    "from analysis_utils import ALG_DISPLAY_NAMES, ALG_TYPES\n",
    "analysis_df.loc[:, \"alg_name\"] = analysis_df[\"alg_name\"].apply(lambda x: ALG_DISPLAY_NAMES[x])\n",
    "\n",
    "# add alg type\n",
    "analysis_df.loc[:, \"alg_type\"] = analysis_df[\"alg_name\"].apply(lambda x: ALG_TYPES[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a copy of each \"default\" hparam row, to treat this as a separate alg\n",
    "default_rows = analysis_df.loc[analysis_df[\"hparam_source\"] == \"default\"].copy()\n",
    "default_rows.loc[:, \"alg_name\"] = default_rows[\"alg_name\"].apply(lambda x: x + \" (default)\")\n",
    "\n",
    "# remove TabPFN and LinearModel, since these only have one hparam set\n",
    "default_rows = default_rows.loc[~(default_rows[\"alg_name\"].str.contains(\"TabPFNModel\") | default_rows[\"alg_name\"].str.contains(\"LinearModel\")), :]\n",
    "\n",
    "# append these to the metadataset\n",
    "analysis_df_with_default = pd.concat([analysis_df, default_rows], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### produce cleaned results files\n",
    "\n",
    "first, each algorithm is tuned for each dataset fold (10 folds per dataset), using all available hyperparameter samples. we then calculate the normalized and ranked performance for each algorithm over all datasets.\n",
    "\n",
    "the following loop produces four files:\n",
    "- `./cleaned_results/tuned_aggregated_results.csv`: performance of each tuned algorithm on each dataset, where performance is averaged over all 10 folds. \n",
    "- `./cleaned_results/tuned_fold_results.csv`: performance of each tuned algorithm on each dataset fold.\n",
    "- `./cleaned_results/tuned_aggregated_results_with_default.csv`: same as `tuned_aggregated_results_with_default.csv`, but with the default hyperparameters of each dataset included as a separate algorithm\n",
    "- `./cleaned_results/tuned_fold_results_with_default.csv`: same as `tuned_fold_results_with_default.csv`, but with the default hyperparameters of each dataset included as a separate algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_result_dfs = {}\n",
    "for drop_default in [True, False]:\n",
    "    for i, (metric, objective_type) in enumerate(zip(metric_list, obj_type_list)):\n",
    "\n",
    "        test_metric_col = metric + \"__test\"\n",
    "\n",
    "        if drop_default:\n",
    "            df = analysis_df.copy()\n",
    "        else:\n",
    "            df = analysis_df_with_default.copy()\n",
    "\n",
    "        tuned_alg_perf = get_tuned_alg_perf(df, metric=metric)\n",
    "\n",
    "        # NOTE: this \"tunes\" each algorithm for each training fold separately. so each of the 10 folds might use different hparams.\n",
    "        tuned_result_dfs[metric] = tuned_alg_perf\n",
    "\n",
    "        ##############################\n",
    "        ### STEP 1: TREAT EACH FOLD AS SEPARATE DATASET\n",
    "\n",
    "        result_col = test_metric_col\n",
    "        \n",
    "        # for each dataset, find the min and max metrics over all tuned algs\n",
    "        overall_bounds = tuned_alg_perf.groupby(\"dataset_fold_id\").agg({result_col: [\"min\", \"max\"]}).reset_index()\n",
    "\n",
    "        # rename the multiindex cols\n",
    "        new_cols = []\n",
    "        for c in overall_bounds.columns:\n",
    "            if c[1] == \"\":\n",
    "                new_cols.append(c[0])\n",
    "            else:\n",
    "                new_cols.append(\"_\".join(c))\n",
    "\n",
    "        overall_bounds.columns = new_cols\n",
    "\n",
    "        tuned_alg_perf = tuned_alg_perf.merge(overall_bounds, on=\"dataset_fold_id\", how=\"left\")\n",
    "\n",
    "        # add normalized metric\n",
    "        tuned_alg_perf.loc[:, \"normalized_\" + result_col] = (tuned_alg_perf[result_col] - tuned_alg_perf[result_col + \"_min\"]) / (tuned_alg_perf[result_col + \"_max\"] - tuned_alg_perf[result_col + \"_min\"])\n",
    "\n",
    "        # rank all algs for each dataset\n",
    "        ascending = False if objective_type == \"maximize\" else True\n",
    "        \n",
    "        tuned_alg_perf.loc[:, f\"{metric}_rank\"] = tuned_alg_perf.groupby([\"dataset_fold_id\"])[result_col].rank(method=\"min\", ascending=ascending).values\n",
    "\n",
    "        # keep these cols to merge\n",
    "        merge_cols = [\n",
    "            \"alg_name\", \n",
    "            \"dataset_fold_id\", \n",
    "            \"normalized_\" + result_col,\n",
    "            f\"{metric}_rank\",\n",
    "            result_col + \"_min\",\n",
    "            result_col + \"_max\"\n",
    "        ]\n",
    "\n",
    "        if i == 0:\n",
    "            fold_tuned_df = tuned_alg_perf.copy()\n",
    "        else:\n",
    "            fold_tuned_df = fold_tuned_df.merge(tuned_alg_perf[merge_cols], on=[\"alg_name\", \"dataset_fold_id\"])\n",
    "\n",
    "        ##############################\n",
    "        ### STEP 2: AVERAGE OVER FOLDS\n",
    "\n",
    "        if i == 0:\n",
    "            agg_dict = {\n",
    "                test_metric_col: [\"median\", \"mean\"],\n",
    "                \"training_time\": [\"median\", \"mean\"],\n",
    "                time_per_inst_col: [\"median\", \"mean\"],\n",
    "                # \"dataset_name\": [\"count\"],\n",
    "            }\n",
    "        else:\n",
    "            agg_dict = {\n",
    "                test_metric_col: [\"median\", \"mean\"],\n",
    "                time_per_inst_col: [\"median\", \"mean\"],\n",
    "            }\n",
    "\n",
    "        # aggregate over folds: take the mean & median performance over each fold\n",
    "        agg_tuned_alg_perf = tuned_alg_perf.groupby([\"alg_name\", \"dataset_name\"]).agg(agg_dict).reset_index()\n",
    "        \n",
    "        # rename the multiindex cols\n",
    "        new_cols = []\n",
    "        for c in agg_tuned_alg_perf.columns:\n",
    "            if c[1] == \"\":\n",
    "                new_cols.append(c[0])\n",
    "            else:\n",
    "                new_cols.append(\"_\".join(c))\n",
    "\n",
    "        agg_tuned_alg_perf.columns = new_cols\n",
    "\n",
    "\n",
    "        # define the target metric column, we will use this value for all plots\n",
    "        result_col = test_metric_col + \"_mean\"\n",
    "\n",
    "        # for each dataset, find the min and max metrics over all tuned algs\n",
    "        overall_bounds = agg_tuned_alg_perf.groupby(\"dataset_name\").agg({result_col: [\"min\", \"max\"]}).reset_index()\n",
    "        \n",
    "        # # normalize metric using these bounds, and get stddev of the mean normalized metric \n",
    "        # tuned_alg_perf = tuned_alg_perf.merge(overall_bounds, on=\"dataset_name\", how=\"left\")\n",
    "        # tuned_alg_perf.loc[:, \"norm_metric_tmp\"] = (tuned_alg_perf[test_metric_col] -  tuned_alg_perf[result_col + \"_min\"]) / (tuned_alg_perf[result_col + \"_max\"] - tuned_alg_perf[result_col + \"_min\"])\n",
    "        # std_metric = tuned_alg_pref.groupby() ...\n",
    "\n",
    "        # rename the multiindex cols\n",
    "        new_cols = []\n",
    "        for c in overall_bounds.columns:\n",
    "            if c[1] == \"\":\n",
    "                new_cols.append(c[0])\n",
    "            else:\n",
    "                new_cols.append(\"_\".join(c))\n",
    "\n",
    "        overall_bounds.columns = new_cols\n",
    "\n",
    "        \n",
    "        agg_tuned_alg_perf = agg_tuned_alg_perf.merge(overall_bounds, on=\"dataset_name\", how=\"left\")\n",
    "\n",
    "        # add normalized metric\n",
    "        agg_tuned_alg_perf.loc[:, \"normalized_\" + result_col] = (agg_tuned_alg_perf[result_col] - agg_tuned_alg_perf[result_col + \"_min\"]) / (agg_tuned_alg_perf[result_col + \"_max\"] - agg_tuned_alg_perf[result_col + \"_min\"])\n",
    "\n",
    "        ###### - new - ######\n",
    "        # estimate the standard deviation of the normalized metric by:\n",
    "        # 1) normalize the metric for all folds using the normalization here\n",
    "        # 2) get std of metric over all folds, like we did to get mean and median above\n",
    "        tmp_df = tuned_alg_perf.loc[:, [\"dataset_name\", \"alg_name\", test_metric_col]].copy()\n",
    "        tmp_df = tmp_df.merge(overall_bounds, on=\"dataset_name\", how=\"left\")    \n",
    "        tmp_df.loc[:, \"normalized_\" + test_metric_col] = (tmp_df[test_metric_col] - tmp_df[result_col + \"_min\"]) / (tmp_df[result_col + \"_max\"] - tmp_df[result_col + \"_min\"])\n",
    "        tmp_agg_df = tmp_df.groupby([\"dataset_name\", \"alg_name\"]).agg({\"normalized_\" + test_metric_col: \"std\"}).reset_index()\n",
    "        tmp_agg_df.columns = [\"dataset_name\", \"alg_name\", \"normalized_\" + test_metric_col + \"_std\"]\n",
    "        agg_tuned_alg_perf = agg_tuned_alg_perf.merge(tmp_agg_df, on=[\"dataset_name\", \"alg_name\"], how=\"left\")\n",
    "\n",
    "        # rank all algs for each dataset\n",
    "        ascending = False if objective_type == \"maximize\" else True\n",
    "        \n",
    "        # rank according to mean performance over all folds\n",
    "        agg_method = \"mean\"\n",
    "\n",
    "        # rank everything\n",
    "        agg_tuned_alg_perf.loc[:, f\"{metric}_rank_{agg_method}\"]  = \\\n",
    "            agg_tuned_alg_perf.groupby([\"dataset_name\"])[test_metric_col + \"_\" + agg_method].rank(method=\"min\", ascending=ascending).values\n",
    "\n",
    "        agg_tuned_alg_perf.rename(columns={\n",
    "            time_per_inst_col + \"_median\": time_per_inst_col + \"_median_\" + metric,\n",
    "            time_per_inst_col + \"_mean\": time_per_inst_col + \"_mean_\" + metric,\n",
    "        }, inplace=True)\n",
    "\n",
    "        # keep these cols to merge\n",
    "        merge_cols = [\n",
    "            \"alg_name\", \n",
    "            \"dataset_name\",\n",
    "            \"normalized_\" + result_col,\n",
    "            \"normalized_\" + test_metric_col + \"_std\",\n",
    "            time_per_inst_col + \"_median_\" + metric,\n",
    "            time_per_inst_col + \"_mean_\" + metric,\n",
    "            f\"{metric}_rank_mean\",\n",
    "            result_col,\n",
    "            result_col + \"_min\",\n",
    "            result_col + \"_max\"\n",
    "        ]\n",
    "\n",
    "        if i == 0:\n",
    "            tuned_agg_df = agg_tuned_alg_perf.copy()\n",
    "        else:\n",
    "            tuned_agg_df = tuned_agg_df.merge(agg_tuned_alg_perf[merge_cols], on=[\"alg_name\", \"dataset_name\"])\n",
    "\n",
    "    # save results\n",
    "\n",
    "    # merge in alg type, for bookkeeping\n",
    "    alg_type_df = analysis_df[[\"alg_name\", \"alg_type\"]].drop_duplicates()\n",
    "    tuned_agg_df = tuned_agg_df.merge(alg_type_df, on=\"alg_name\", how=\"left\")\n",
    "    fold_tuned_df = fold_tuned_df.merge(alg_type_df, on=\"alg_name\", how=\"left\")\n",
    "\n",
    "    if drop_default:\n",
    "        agg_df_no_default = tuned_agg_df.copy()\n",
    "        agg_df_no_default.to_csv(\"./cleaned_results/tuned_aggregated_results.csv\")\n",
    "\n",
    "        tuned_fold_df_no_default = fold_tuned_df.copy()\n",
    "        tuned_fold_df_no_default.to_csv(\"./cleaned_results/tuned_fold_results.csv\")\n",
    "       \n",
    "    else:\n",
    "        agg_df_with_default = tuned_agg_df.copy()\n",
    "        agg_df_with_default.to_csv(\"./cleaned_results/tuned_aggregated_results_with_default.csv\")\n",
    "\n",
    "        tuned_fold_df_with_default = fold_tuned_df.copy()\n",
    "        tuned_fold_df_with_default.to_csv(\"./cleaned_results/tuned_fold_results_with_default.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>Accuracy__test_median</th>\n",
       "      <th>Accuracy__test_mean</th>\n",
       "      <th>training_time_median</th>\n",
       "      <th>training_time_mean</th>\n",
       "      <th>train_per_1000_inst_median_Accuracy</th>\n",
       "      <th>train_per_1000_inst_mean_Accuracy</th>\n",
       "      <th>Accuracy__test_mean_min</th>\n",
       "      <th>Accuracy__test_mean_max</th>\n",
       "      <th>...</th>\n",
       "      <th>Log Loss__test_mean_max</th>\n",
       "      <th>normalized_AUC__test_mean</th>\n",
       "      <th>normalized_AUC__test_std</th>\n",
       "      <th>train_per_1000_inst_median_AUC</th>\n",
       "      <th>train_per_1000_inst_mean_AUC</th>\n",
       "      <th>AUC_rank_mean</th>\n",
       "      <th>AUC__test_mean</th>\n",
       "      <th>AUC__test_mean_min</th>\n",
       "      <th>AUC__test_mean_max</th>\n",
       "      <th>alg_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__APSFailure__168868</td>\n",
       "      <td>0.994145</td>\n",
       "      <td>0.994013</td>\n",
       "      <td>6.412328</td>\n",
       "      <td>7.276401</td>\n",
       "      <td>0.105466</td>\n",
       "      <td>0.119678</td>\n",
       "      <td>0.970303</td>\n",
       "      <td>0.994500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.186996</td>\n",
       "      <td>0.143336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991724</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.991724</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Amazon_employee_access__34539</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.947359</td>\n",
       "      <td>1.708439</td>\n",
       "      <td>1.729567</td>\n",
       "      <td>0.065169</td>\n",
       "      <td>0.065976</td>\n",
       "      <td>0.928927</td>\n",
       "      <td>0.952150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.020650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.065087</td>\n",
       "      <td>0.075518</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.862513</td>\n",
       "      <td>0.492269</td>\n",
       "      <td>0.862513</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Australian__146818</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>1.347650</td>\n",
       "      <td>1.393643</td>\n",
       "      <td>2.441396</td>\n",
       "      <td>2.524716</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682198</td>\n",
       "      <td>0.992585</td>\n",
       "      <td>0.100835</td>\n",
       "      <td>2.114068</td>\n",
       "      <td>1.998640</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>0.759653</td>\n",
       "      <td>0.943688</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Bioresponse__9910</td>\n",
       "      <td>0.798940</td>\n",
       "      <td>0.795521</td>\n",
       "      <td>5.815126</td>\n",
       "      <td>6.748842</td>\n",
       "      <td>1.937729</td>\n",
       "      <td>2.248931</td>\n",
       "      <td>0.490545</td>\n",
       "      <td>0.796848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967133</td>\n",
       "      <td>0.971437</td>\n",
       "      <td>0.050844</td>\n",
       "      <td>2.237972</td>\n",
       "      <td>2.143907</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>0.495182</td>\n",
       "      <td>0.876545</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Census-Income__168340</td>\n",
       "      <td>0.958869</td>\n",
       "      <td>0.958658</td>\n",
       "      <td>830.825629</td>\n",
       "      <td>857.921920</td>\n",
       "      <td>3.470058</td>\n",
       "      <td>3.583213</td>\n",
       "      <td>0.943238</td>\n",
       "      <td>0.958658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474823</td>\n",
       "      <td>0.992560</td>\n",
       "      <td>0.018931</td>\n",
       "      <td>4.932732</td>\n",
       "      <td>4.874733</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.955365</td>\n",
       "      <td>0.852212</td>\n",
       "      <td>0.956139</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alg_name                           dataset_name  Accuracy__test_median  \\\n",
       "0  CatBoost             openml__APSFailure__168868               0.994145   \n",
       "1  CatBoost  openml__Amazon_employee_access__34539               0.946903   \n",
       "2  CatBoost             openml__Australian__146818               0.869565   \n",
       "3  CatBoost              openml__Bioresponse__9910               0.798940   \n",
       "4  CatBoost          openml__Census-Income__168340               0.958869   \n",
       "\n",
       "   Accuracy__test_mean  training_time_median  training_time_mean  \\\n",
       "0             0.994013              6.412328            7.276401   \n",
       "1             0.947359              1.708439            1.729567   \n",
       "2             0.872464              1.347650            1.393643   \n",
       "3             0.795521              5.815126            6.748842   \n",
       "4             0.958658            830.825629          857.921920   \n",
       "\n",
       "   train_per_1000_inst_median_Accuracy  train_per_1000_inst_mean_Accuracy  \\\n",
       "0                             0.105466                           0.119678   \n",
       "1                             0.065169                           0.065976   \n",
       "2                             2.441396                           2.524716   \n",
       "3                             1.937729                           2.248931   \n",
       "4                             3.470058                           3.583213   \n",
       "\n",
       "   Accuracy__test_mean_min  Accuracy__test_mean_max  ...  \\\n",
       "0                 0.970303                 0.994500  ...   \n",
       "1                 0.928927                 0.952150  ...   \n",
       "2                 0.601449                 0.872464  ...   \n",
       "3                 0.490545                 0.796848  ...   \n",
       "4                 0.943238                 0.958658  ...   \n",
       "\n",
       "   Log Loss__test_mean_max  normalized_AUC__test_mean  \\\n",
       "0                 0.624879                   1.000000   \n",
       "1                 1.020650                   1.000000   \n",
       "2                 1.682198                   0.992585   \n",
       "3                 0.967133                   0.971437   \n",
       "4                 0.474823                   0.992560   \n",
       "\n",
       "   normalized_AUC__test_std  train_per_1000_inst_median_AUC  \\\n",
       "0                  0.004881                        0.186996   \n",
       "1                  0.035643                        0.065087   \n",
       "2                  0.100835                        2.114068   \n",
       "3                  0.050844                        2.237972   \n",
       "4                  0.018931                        4.932732   \n",
       "\n",
       "   train_per_1000_inst_mean_AUC  AUC_rank_mean  AUC__test_mean  \\\n",
       "0                      0.143336            1.0        0.991724   \n",
       "1                      0.075518            1.0        0.862513   \n",
       "2                      1.998640            2.0        0.942324   \n",
       "3                      2.143907            5.0        0.865652   \n",
       "4                      4.874733            3.0        0.955365   \n",
       "\n",
       "   AUC__test_mean_min  AUC__test_mean_max  alg_type  \n",
       "0            0.500000            0.991724      gbdt  \n",
       "1            0.492269            0.862513      gbdt  \n",
       "2            0.759653            0.943688      gbdt  \n",
       "3            0.495182            0.876545      gbdt  \n",
       "4            0.852212            0.956139      gbdt  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek\n",
    "tuned_agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4094171/525844158.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  tuned_agg_df[tuned_agg_df[\"dataset_name\"].isin(DATASETS)][tuned_agg_df['alg_name'].isin(['CatBoost', 'TabFlex', 'TabPFN'])]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>Accuracy__test_median</th>\n",
       "      <th>Accuracy__test_mean</th>\n",
       "      <th>training_time_median</th>\n",
       "      <th>training_time_mean</th>\n",
       "      <th>train_per_1000_inst_median_Accuracy</th>\n",
       "      <th>train_per_1000_inst_mean_Accuracy</th>\n",
       "      <th>Accuracy__test_mean_min</th>\n",
       "      <th>Accuracy__test_mean_max</th>\n",
       "      <th>...</th>\n",
       "      <th>Log Loss__test_mean_max</th>\n",
       "      <th>normalized_AUC__test_mean</th>\n",
       "      <th>normalized_AUC__test_std</th>\n",
       "      <th>train_per_1000_inst_median_AUC</th>\n",
       "      <th>train_per_1000_inst_mean_AUC</th>\n",
       "      <th>AUC_rank_mean</th>\n",
       "      <th>AUC__test_mean</th>\n",
       "      <th>AUC__test_mean_min</th>\n",
       "      <th>AUC__test_mean_max</th>\n",
       "      <th>alg_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Australian__146818</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>1.347650</td>\n",
       "      <td>1.393643</td>\n",
       "      <td>2.441396</td>\n",
       "      <td>2.524716</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682198</td>\n",
       "      <td>0.992585</td>\n",
       "      <td>0.100835</td>\n",
       "      <td>2.114068</td>\n",
       "      <td>1.998640</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>0.759653</td>\n",
       "      <td>0.943688</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__Bioresponse__9910</td>\n",
       "      <td>0.798940</td>\n",
       "      <td>0.795521</td>\n",
       "      <td>5.815126</td>\n",
       "      <td>6.748842</td>\n",
       "      <td>1.937729</td>\n",
       "      <td>2.248931</td>\n",
       "      <td>0.490545</td>\n",
       "      <td>0.796848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967133</td>\n",
       "      <td>0.971437</td>\n",
       "      <td>0.050844</td>\n",
       "      <td>2.237972</td>\n",
       "      <td>2.143907</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.865652</td>\n",
       "      <td>0.495182</td>\n",
       "      <td>0.876545</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__GesturePhaseSegmentationProcessed__14969</td>\n",
       "      <td>0.615502</td>\n",
       "      <td>0.616022</td>\n",
       "      <td>11.644987</td>\n",
       "      <td>7.844962</td>\n",
       "      <td>1.474330</td>\n",
       "      <td>0.993208</td>\n",
       "      <td>0.298795</td>\n",
       "      <td>0.699889</td>\n",
       "      <td>...</td>\n",
       "      <td>5.715575</td>\n",
       "      <td>0.899894</td>\n",
       "      <td>0.019957</td>\n",
       "      <td>0.879405</td>\n",
       "      <td>0.800541</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.862134</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.902419</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__MiniBooNE__168335</td>\n",
       "      <td>0.942757</td>\n",
       "      <td>0.942813</td>\n",
       "      <td>8.844148</td>\n",
       "      <td>7.955771</td>\n",
       "      <td>0.084998</td>\n",
       "      <td>0.076460</td>\n",
       "      <td>0.747838</td>\n",
       "      <td>0.946226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>0.985543</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.085092</td>\n",
       "      <td>0.085040</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.984494</td>\n",
       "      <td>0.907805</td>\n",
       "      <td>0.985619</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__SpeedDating__146607</td>\n",
       "      <td>0.867542</td>\n",
       "      <td>0.866318</td>\n",
       "      <td>20.614545</td>\n",
       "      <td>34.475734</td>\n",
       "      <td>3.075481</td>\n",
       "      <td>5.143934</td>\n",
       "      <td>0.799234</td>\n",
       "      <td>0.871090</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027214</td>\n",
       "      <td>0.959466</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>6.341648</td>\n",
       "      <td>10.479124</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.869733</td>\n",
       "      <td>0.679571</td>\n",
       "      <td>0.877767</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__ada_agnostic__3896</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.854230</td>\n",
       "      <td>0.406390</td>\n",
       "      <td>0.986107</td>\n",
       "      <td>0.111373</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.751864</td>\n",
       "      <td>0.857521</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067645</td>\n",
       "      <td>0.084726</td>\n",
       "      <td>0.183536</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.906147</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.906147</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__airlines__189354</td>\n",
       "      <td>0.663289</td>\n",
       "      <td>0.663061</td>\n",
       "      <td>6.920802</td>\n",
       "      <td>6.949461</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.569102</td>\n",
       "      <td>0.671467</td>\n",
       "      <td>...</td>\n",
       "      <td>1.075940</td>\n",
       "      <td>0.933155</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.714689</td>\n",
       "      <td>0.573717</td>\n",
       "      <td>0.724788</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__albert__189356</td>\n",
       "      <td>0.703379</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>33.092527</td>\n",
       "      <td>35.153920</td>\n",
       "      <td>0.097276</td>\n",
       "      <td>0.103336</td>\n",
       "      <td>0.563360</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013941</td>\n",
       "      <td>0.096953</td>\n",
       "      <td>0.097484</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.776825</td>\n",
       "      <td>0.593685</td>\n",
       "      <td>0.776825</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__artificial-characters__14964</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>0.854570</td>\n",
       "      <td>12.514503</td>\n",
       "      <td>12.465605</td>\n",
       "      <td>1.530976</td>\n",
       "      <td>1.524957</td>\n",
       "      <td>0.317188</td>\n",
       "      <td>0.954493</td>\n",
       "      <td>...</td>\n",
       "      <td>6.514097</td>\n",
       "      <td>0.953854</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>1.530976</td>\n",
       "      <td>1.524957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.988423</td>\n",
       "      <td>0.790613</td>\n",
       "      <td>0.997992</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__audiology__7</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.752372</td>\n",
       "      <td>5.370374</td>\n",
       "      <td>159.025557</td>\n",
       "      <td>29.753904</td>\n",
       "      <td>878.758593</td>\n",
       "      <td>0.260474</td>\n",
       "      <td>0.800988</td>\n",
       "      <td>...</td>\n",
       "      <td>8.816495</td>\n",
       "      <td>0.814579</td>\n",
       "      <td>0.234645</td>\n",
       "      <td>18.259061</td>\n",
       "      <td>1098.535194</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.877642</td>\n",
       "      <td>0.531318</td>\n",
       "      <td>0.956476</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__balance-scale__11</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.907220</td>\n",
       "      <td>0.986136</td>\n",
       "      <td>0.994758</td>\n",
       "      <td>1.972637</td>\n",
       "      <td>1.989965</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.990348</td>\n",
       "      <td>...</td>\n",
       "      <td>2.533980</td>\n",
       "      <td>0.816782</td>\n",
       "      <td>0.082053</td>\n",
       "      <td>1.892889</td>\n",
       "      <td>1.934961</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.950065</td>\n",
       "      <td>0.730546</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__cnae-9__9981</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.923148</td>\n",
       "      <td>1.841153</td>\n",
       "      <td>1.796659</td>\n",
       "      <td>2.130964</td>\n",
       "      <td>2.079466</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.957407</td>\n",
       "      <td>...</td>\n",
       "      <td>1.467915</td>\n",
       "      <td>0.980417</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>2.134351</td>\n",
       "      <td>2.979422</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.994483</td>\n",
       "      <td>0.811468</td>\n",
       "      <td>0.998139</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__colic__25</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.864114</td>\n",
       "      <td>0.813607</td>\n",
       "      <td>2.897697</td>\n",
       "      <td>2.761901</td>\n",
       "      <td>9.853124</td>\n",
       "      <td>0.627928</td>\n",
       "      <td>0.872297</td>\n",
       "      <td>...</td>\n",
       "      <td>1.503416</td>\n",
       "      <td>0.980441</td>\n",
       "      <td>0.156481</td>\n",
       "      <td>2.827953</td>\n",
       "      <td>4.881225</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.904803</td>\n",
       "      <td>0.562650</td>\n",
       "      <td>0.911629</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__credit-approval__29</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.875362</td>\n",
       "      <td>1.388361</td>\n",
       "      <td>1.517310</td>\n",
       "      <td>2.515146</td>\n",
       "      <td>2.748751</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.355583</td>\n",
       "      <td>0.976911</td>\n",
       "      <td>0.145686</td>\n",
       "      <td>1.597309</td>\n",
       "      <td>1.721987</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.938801</td>\n",
       "      <td>0.692580</td>\n",
       "      <td>0.944620</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__credit-g__31</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>1.605036</td>\n",
       "      <td>1.740370</td>\n",
       "      <td>2.006295</td>\n",
       "      <td>2.175462</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489243</td>\n",
       "      <td>0.928016</td>\n",
       "      <td>0.185722</td>\n",
       "      <td>2.361788</td>\n",
       "      <td>3.148109</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.774905</td>\n",
       "      <td>0.507857</td>\n",
       "      <td>0.795619</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__electricity__219</td>\n",
       "      <td>0.859965</td>\n",
       "      <td>0.860015</td>\n",
       "      <td>3.528122</td>\n",
       "      <td>3.539238</td>\n",
       "      <td>0.097330</td>\n",
       "      <td>0.097635</td>\n",
       "      <td>0.625287</td>\n",
       "      <td>0.938140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677321</td>\n",
       "      <td>0.848551</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.097330</td>\n",
       "      <td>0.097635</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.936853</td>\n",
       "      <td>0.669873</td>\n",
       "      <td>0.984503</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__elevators__3711</td>\n",
       "      <td>0.895181</td>\n",
       "      <td>0.895657</td>\n",
       "      <td>3.186074</td>\n",
       "      <td>3.743139</td>\n",
       "      <td>0.239933</td>\n",
       "      <td>0.281881</td>\n",
       "      <td>0.686187</td>\n",
       "      <td>0.903127</td>\n",
       "      <td>...</td>\n",
       "      <td>1.016360</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>0.011247</td>\n",
       "      <td>0.222029</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.944335</td>\n",
       "      <td>0.574184</td>\n",
       "      <td>0.949972</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__heart-h__50</td>\n",
       "      <td>0.830460</td>\n",
       "      <td>0.833103</td>\n",
       "      <td>0.235890</td>\n",
       "      <td>0.773598</td>\n",
       "      <td>1.001103</td>\n",
       "      <td>3.296120</td>\n",
       "      <td>0.639540</td>\n",
       "      <td>0.843448</td>\n",
       "      <td>...</td>\n",
       "      <td>3.639566</td>\n",
       "      <td>0.901218</td>\n",
       "      <td>0.167460</td>\n",
       "      <td>0.906026</td>\n",
       "      <td>2.166365</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.886350</td>\n",
       "      <td>0.629070</td>\n",
       "      <td>0.914551</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__higgs__146606</td>\n",
       "      <td>0.724477</td>\n",
       "      <td>0.724029</td>\n",
       "      <td>2.217028</td>\n",
       "      <td>4.298315</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>0.054797</td>\n",
       "      <td>0.640622</td>\n",
       "      <td>0.732004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639220</td>\n",
       "      <td>0.917294</td>\n",
       "      <td>0.055919</td>\n",
       "      <td>0.027851</td>\n",
       "      <td>0.044075</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.802189</td>\n",
       "      <td>0.682077</td>\n",
       "      <td>0.813019</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__jasmine__168911</td>\n",
       "      <td>0.817423</td>\n",
       "      <td>0.816006</td>\n",
       "      <td>1.930358</td>\n",
       "      <td>2.099163</td>\n",
       "      <td>0.808358</td>\n",
       "      <td>0.879336</td>\n",
       "      <td>0.746014</td>\n",
       "      <td>0.816006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841618</td>\n",
       "      <td>0.903119</td>\n",
       "      <td>0.379115</td>\n",
       "      <td>0.716898</td>\n",
       "      <td>0.936770</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.868502</td>\n",
       "      <td>0.822619</td>\n",
       "      <td>0.873424</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__jungle_chess_2pcs_raw_endgame_complete...</td>\n",
       "      <td>0.865558</td>\n",
       "      <td>0.866240</td>\n",
       "      <td>1.620528</td>\n",
       "      <td>1.602494</td>\n",
       "      <td>0.045197</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.712198</td>\n",
       "      <td>0.950958</td>\n",
       "      <td>...</td>\n",
       "      <td>2.230348</td>\n",
       "      <td>0.834268</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>0.172652</td>\n",
       "      <td>0.148111</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.969604</td>\n",
       "      <td>0.836496</td>\n",
       "      <td>0.996047</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__kc1__3917</td>\n",
       "      <td>0.862559</td>\n",
       "      <td>0.859652</td>\n",
       "      <td>1.484592</td>\n",
       "      <td>1.848915</td>\n",
       "      <td>0.879763</td>\n",
       "      <td>1.095877</td>\n",
       "      <td>0.776705</td>\n",
       "      <td>0.862013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.136808</td>\n",
       "      <td>0.951763</td>\n",
       "      <td>0.132173</td>\n",
       "      <td>1.274706</td>\n",
       "      <td>1.433158</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.814911</td>\n",
       "      <td>0.559898</td>\n",
       "      <td>0.827835</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__lymph__10</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>2.070939</td>\n",
       "      <td>10.032889</td>\n",
       "      <td>17.392246</td>\n",
       "      <td>84.964077</td>\n",
       "      <td>0.269524</td>\n",
       "      <td>0.844286</td>\n",
       "      <td>...</td>\n",
       "      <td>5.534808</td>\n",
       "      <td>0.874895</td>\n",
       "      <td>0.259187</td>\n",
       "      <td>31.704225</td>\n",
       "      <td>98.856143</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.885764</td>\n",
       "      <td>0.506961</td>\n",
       "      <td>0.939931</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__mfeat-fourier__14</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>4.127188</td>\n",
       "      <td>7.445625</td>\n",
       "      <td>2.579493</td>\n",
       "      <td>4.653516</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.553028</td>\n",
       "      <td>0.925951</td>\n",
       "      <td>0.055302</td>\n",
       "      <td>1.683642</td>\n",
       "      <td>1.702867</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.982978</td>\n",
       "      <td>0.913231</td>\n",
       "      <td>0.988556</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__mfeat-zernike__22</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>1.595188</td>\n",
       "      <td>1.505634</td>\n",
       "      <td>0.996992</td>\n",
       "      <td>0.941021</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>...</td>\n",
       "      <td>6.847110</td>\n",
       "      <td>0.895176</td>\n",
       "      <td>0.025367</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.852579</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.976364</td>\n",
       "      <td>0.868739</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__monks-problems-2__146065</td>\n",
       "      <td>0.933880</td>\n",
       "      <td>0.930109</td>\n",
       "      <td>2.288369</td>\n",
       "      <td>5.670781</td>\n",
       "      <td>4.757523</td>\n",
       "      <td>11.793725</td>\n",
       "      <td>0.637350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743279</td>\n",
       "      <td>0.962354</td>\n",
       "      <td>0.056971</td>\n",
       "      <td>6.674921</td>\n",
       "      <td>11.681131</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.981415</td>\n",
       "      <td>0.506336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__nomao__9977</td>\n",
       "      <td>0.966778</td>\n",
       "      <td>0.967880</td>\n",
       "      <td>15.815547</td>\n",
       "      <td>14.697113</td>\n",
       "      <td>0.573609</td>\n",
       "      <td>0.533045</td>\n",
       "      <td>0.938691</td>\n",
       "      <td>0.973306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306727</td>\n",
       "      <td>0.963238</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.573609</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.994576</td>\n",
       "      <td>0.960179</td>\n",
       "      <td>0.995889</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__one-hundred-plants-texture__9956</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.846808</td>\n",
       "      <td>724.769250</td>\n",
       "      <td>498.854478</td>\n",
       "      <td>566.500631</td>\n",
       "      <td>389.977908</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.846808</td>\n",
       "      <td>...</td>\n",
       "      <td>20.778965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>566.558735</td>\n",
       "      <td>526.507768</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997197</td>\n",
       "      <td>0.545225</td>\n",
       "      <td>0.997197</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__phoneme__9952</td>\n",
       "      <td>0.881701</td>\n",
       "      <td>0.881941</td>\n",
       "      <td>0.813439</td>\n",
       "      <td>1.107313</td>\n",
       "      <td>0.188122</td>\n",
       "      <td>0.256140</td>\n",
       "      <td>0.716878</td>\n",
       "      <td>0.908032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580233</td>\n",
       "      <td>0.944094</td>\n",
       "      <td>0.034245</td>\n",
       "      <td>0.248012</td>\n",
       "      <td>0.284848</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.945472</td>\n",
       "      <td>0.678465</td>\n",
       "      <td>0.961283</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__poker-hand__9890</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.905178</td>\n",
       "      <td>16.938304</td>\n",
       "      <td>16.746106</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>0.304311</td>\n",
       "      <td>0.932046</td>\n",
       "      <td>...</td>\n",
       "      <td>3.277077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>1.463282</td>\n",
       "      <td>1.466485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933543</td>\n",
       "      <td>0.538271</td>\n",
       "      <td>0.933543</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__profb__3561</td>\n",
       "      <td>0.725966</td>\n",
       "      <td>0.726251</td>\n",
       "      <td>0.834443</td>\n",
       "      <td>0.768667</td>\n",
       "      <td>1.551010</td>\n",
       "      <td>1.429437</td>\n",
       "      <td>0.607199</td>\n",
       "      <td>0.744030</td>\n",
       "      <td>...</td>\n",
       "      <td>2.137502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.297807</td>\n",
       "      <td>0.866363</td>\n",
       "      <td>1.206552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772984</td>\n",
       "      <td>0.478791</td>\n",
       "      <td>0.772984</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__qsar-biodeg__9957</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.864438</td>\n",
       "      <td>1.245820</td>\n",
       "      <td>1.200767</td>\n",
       "      <td>1.476182</td>\n",
       "      <td>1.422597</td>\n",
       "      <td>0.666379</td>\n",
       "      <td>0.885328</td>\n",
       "      <td>...</td>\n",
       "      <td>1.460949</td>\n",
       "      <td>0.877756</td>\n",
       "      <td>0.215603</td>\n",
       "      <td>1.595759</td>\n",
       "      <td>1.976704</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.930141</td>\n",
       "      <td>0.812509</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__socmob__3797</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.951559</td>\n",
       "      <td>0.674109</td>\n",
       "      <td>0.889292</td>\n",
       "      <td>0.728782</td>\n",
       "      <td>0.961540</td>\n",
       "      <td>0.778561</td>\n",
       "      <td>0.955015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752412</td>\n",
       "      <td>0.972811</td>\n",
       "      <td>0.053618</td>\n",
       "      <td>0.390695</td>\n",
       "      <td>0.619440</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.985848</td>\n",
       "      <td>0.834043</td>\n",
       "      <td>0.990091</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__splice__45</td>\n",
       "      <td>0.959248</td>\n",
       "      <td>0.959248</td>\n",
       "      <td>2.701476</td>\n",
       "      <td>2.582958</td>\n",
       "      <td>1.058572</td>\n",
       "      <td>1.012131</td>\n",
       "      <td>0.550470</td>\n",
       "      <td>0.967398</td>\n",
       "      <td>...</td>\n",
       "      <td>9.403385</td>\n",
       "      <td>0.995352</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.976559</td>\n",
       "      <td>1.049103</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.992461</td>\n",
       "      <td>0.571933</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__vehicle__53</td>\n",
       "      <td>0.763305</td>\n",
       "      <td>0.768347</td>\n",
       "      <td>1.386822</td>\n",
       "      <td>2.460016</td>\n",
       "      <td>2.048725</td>\n",
       "      <td>3.634975</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>0.847493</td>\n",
       "      <td>...</td>\n",
       "      <td>12.908916</td>\n",
       "      <td>0.913797</td>\n",
       "      <td>0.025148</td>\n",
       "      <td>2.036000</td>\n",
       "      <td>3.317002</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.934469</td>\n",
       "      <td>0.570317</td>\n",
       "      <td>0.968821</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__Australian__146818</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.872464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682198</td>\n",
       "      <td>0.936633</td>\n",
       "      <td>0.132862</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.932026</td>\n",
       "      <td>0.759653</td>\n",
       "      <td>0.943688</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__balance-scale__11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990348</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.990348</td>\n",
       "      <td>...</td>\n",
       "      <td>2.533980</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.999195</td>\n",
       "      <td>0.730546</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__colic__25</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.823123</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.627928</td>\n",
       "      <td>0.872297</td>\n",
       "      <td>...</td>\n",
       "      <td>1.503416</td>\n",
       "      <td>0.903773</td>\n",
       "      <td>0.126267</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.878048</td>\n",
       "      <td>0.562650</td>\n",
       "      <td>0.911629</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__credit-approval__29</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.879710</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.355583</td>\n",
       "      <td>0.960585</td>\n",
       "      <td>0.143565</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.934686</td>\n",
       "      <td>0.692580</td>\n",
       "      <td>0.944620</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__credit-g__31</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489243</td>\n",
       "      <td>0.899884</td>\n",
       "      <td>0.130745</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.766810</td>\n",
       "      <td>0.507857</td>\n",
       "      <td>0.795619</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5584</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__heart-h__50</td>\n",
       "      <td>0.830460</td>\n",
       "      <td>0.833448</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.639540</td>\n",
       "      <td>0.843448</td>\n",
       "      <td>...</td>\n",
       "      <td>3.639566</td>\n",
       "      <td>0.883443</td>\n",
       "      <td>0.170938</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.881276</td>\n",
       "      <td>0.629070</td>\n",
       "      <td>0.914551</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__lymph__10</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.269524</td>\n",
       "      <td>0.844286</td>\n",
       "      <td>...</td>\n",
       "      <td>5.534808</td>\n",
       "      <td>0.921676</td>\n",
       "      <td>0.176856</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.906019</td>\n",
       "      <td>0.506961</td>\n",
       "      <td>0.939931</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__monks-problems-2__146065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.637350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5598</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__profb__3561</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.690540</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.607199</td>\n",
       "      <td>0.744030</td>\n",
       "      <td>...</td>\n",
       "      <td>2.137502</td>\n",
       "      <td>0.635560</td>\n",
       "      <td>0.144438</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.665769</td>\n",
       "      <td>0.478791</td>\n",
       "      <td>0.772984</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5599</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__qsar-biodeg__9957</td>\n",
       "      <td>0.891015</td>\n",
       "      <td>0.884376</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.666379</td>\n",
       "      <td>0.885328</td>\n",
       "      <td>...</td>\n",
       "      <td>1.460949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.223149</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>0.812509</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5601</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__socmob__3797</td>\n",
       "      <td>0.935082</td>\n",
       "      <td>0.932564</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.778561</td>\n",
       "      <td>0.955015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752412</td>\n",
       "      <td>0.926726</td>\n",
       "      <td>0.075297</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.978656</td>\n",
       "      <td>0.834043</td>\n",
       "      <td>0.990091</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>openml__vehicle__53</td>\n",
       "      <td>0.846148</td>\n",
       "      <td>0.842745</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>0.847493</td>\n",
       "      <td>...</td>\n",
       "      <td>12.908916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968821</td>\n",
       "      <td>0.570317</td>\n",
       "      <td>0.968821</td>\n",
       "      <td>pfn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alg_name                                       dataset_name  \\\n",
       "2     CatBoost                         openml__Australian__146818   \n",
       "3     CatBoost                          openml__Bioresponse__9910   \n",
       "7     CatBoost   openml__GesturePhaseSegmentationProcessed__14969   \n",
       "13    CatBoost                          openml__MiniBooNE__168335   \n",
       "16    CatBoost                        openml__SpeedDating__146607   \n",
       "18    CatBoost                         openml__ada_agnostic__3896   \n",
       "21    CatBoost                           openml__airlines__189354   \n",
       "22    CatBoost                             openml__albert__189356   \n",
       "29    CatBoost               openml__artificial-characters__14964   \n",
       "30    CatBoost                               openml__audiology__7   \n",
       "32    CatBoost                          openml__balance-scale__11   \n",
       "49    CatBoost                               openml__cnae-9__9981   \n",
       "50    CatBoost                                  openml__colic__25   \n",
       "54    CatBoost                        openml__credit-approval__29   \n",
       "55    CatBoost                               openml__credit-g__31   \n",
       "64    CatBoost                           openml__electricity__219   \n",
       "65    CatBoost                            openml__elevators__3711   \n",
       "82    CatBoost                                openml__heart-h__50   \n",
       "84    CatBoost                              openml__higgs__146606   \n",
       "92    CatBoost                            openml__jasmine__168911   \n",
       "94    CatBoost  openml__jungle_chess_2pcs_raw_endgame_complete...   \n",
       "95    CatBoost                                  openml__kc1__3917   \n",
       "104   CatBoost                                  openml__lymph__10   \n",
       "108   CatBoost                          openml__mfeat-fourier__14   \n",
       "112   CatBoost                          openml__mfeat-zernike__22   \n",
       "114   CatBoost                   openml__monks-problems-2__146065   \n",
       "117   CatBoost                                openml__nomao__9977   \n",
       "120   CatBoost           openml__one-hundred-plants-texture__9956   \n",
       "128   CatBoost                              openml__phoneme__9952   \n",
       "129   CatBoost                           openml__poker-hand__9890   \n",
       "133   CatBoost                                openml__profb__3561   \n",
       "134   CatBoost                          openml__qsar-biodeg__9957   \n",
       "142   CatBoost                               openml__socmob__3797   \n",
       "146   CatBoost                                 openml__splice__45   \n",
       "155   CatBoost                                openml__vehicle__53   \n",
       "5550    TabPFN                         openml__Australian__146818   \n",
       "5560    TabPFN                          openml__balance-scale__11   \n",
       "5566    TabPFN                                  openml__colic__25   \n",
       "5568    TabPFN                        openml__credit-approval__29   \n",
       "5569    TabPFN                               openml__credit-g__31   \n",
       "5584    TabPFN                                openml__heart-h__50   \n",
       "5594    TabPFN                                  openml__lymph__10   \n",
       "5595    TabPFN                   openml__monks-problems-2__146065   \n",
       "5598    TabPFN                                openml__profb__3561   \n",
       "5599    TabPFN                          openml__qsar-biodeg__9957   \n",
       "5601    TabPFN                               openml__socmob__3797   \n",
       "5608    TabPFN                                openml__vehicle__53   \n",
       "\n",
       "      Accuracy__test_median  Accuracy__test_mean  training_time_median  \\\n",
       "2                  0.869565             0.872464              1.347650   \n",
       "3                  0.798940             0.795521              5.815126   \n",
       "7                  0.615502             0.616022             11.644987   \n",
       "13                 0.942757             0.942813              8.844148   \n",
       "16                 0.867542             0.866318             20.614545   \n",
       "18                 0.855422             0.854230              0.406390   \n",
       "21                 0.663289             0.663061              6.920802   \n",
       "22                 0.703379             0.704762             33.092527   \n",
       "29                 0.856164             0.854570             12.514503   \n",
       "30                 0.739130             0.752372              5.370374   \n",
       "32                 0.919355             0.907220              0.986136   \n",
       "49                 0.930556             0.923148              1.841153   \n",
       "50                 0.861111             0.864114              0.813607   \n",
       "54                 0.884058             0.875362              1.388361   \n",
       "55                 0.745000             0.746000              1.605036   \n",
       "64                 0.859965             0.860015              3.528122   \n",
       "65                 0.895181             0.895657              3.186074   \n",
       "82                 0.830460             0.833103              0.235890   \n",
       "84                 0.724477             0.724029              2.217028   \n",
       "92                 0.817423             0.816006              1.930358   \n",
       "94                 0.865558             0.866240              1.620528   \n",
       "95                 0.862559             0.859652              1.484592   \n",
       "104                0.800000             0.809524              2.070939   \n",
       "108                0.832500             0.824000              4.127188   \n",
       "112                0.772500             0.772500              1.595188   \n",
       "114                0.933880             0.930109              2.288369   \n",
       "117                0.966778             0.967880             15.815547   \n",
       "120                0.850000             0.846808            724.769250   \n",
       "128                0.881701             0.881941              0.813439   \n",
       "129                0.904459             0.905178             16.938304   \n",
       "133                0.725966             0.726251              0.834443   \n",
       "134                0.857143             0.864438              1.245820   \n",
       "142                0.956522             0.951559              0.674109   \n",
       "146                0.959248             0.959248              2.701476   \n",
       "155                0.763305             0.768347              1.386822   \n",
       "5550               0.855072             0.866667              0.000661   \n",
       "5560               1.000000             0.990348              0.000371   \n",
       "5566               0.824324             0.823123              0.000611   \n",
       "5568               0.884058             0.879710              0.000633   \n",
       "5569               0.730000             0.730000              0.000922   \n",
       "5584               0.830460             0.833448              0.000475   \n",
       "5594               0.800000             0.802857              0.000407   \n",
       "5595               1.000000             1.000000              0.000474   \n",
       "5598               0.686567             0.690540              0.000543   \n",
       "5599               0.891015             0.884376              0.000372   \n",
       "5601               0.935082             0.932564              0.000539   \n",
       "5608               0.846148             0.842745              0.000383   \n",
       "\n",
       "      training_time_mean  train_per_1000_inst_median_Accuracy  \\\n",
       "2               1.393643                             2.441396   \n",
       "3               6.748842                             1.937729   \n",
       "7               7.844962                             1.474330   \n",
       "13              7.955771                             0.084998   \n",
       "16             34.475734                             3.075481   \n",
       "18              0.986107                             0.111373   \n",
       "21              6.949461                             0.016039   \n",
       "22             35.153920                             0.097276   \n",
       "29             12.465605                             1.530976   \n",
       "30            159.025557                            29.753904   \n",
       "32              0.994758                             1.972637   \n",
       "49              1.796659                             2.130964   \n",
       "50              2.897697                             2.761901   \n",
       "54              1.517310                             2.515146   \n",
       "55              1.740370                             2.006295   \n",
       "64              3.539238                             0.097330   \n",
       "65              3.743139                             0.239933   \n",
       "82              0.773598                             1.001103   \n",
       "84              4.298315                             0.028264   \n",
       "92              2.099163                             0.808358   \n",
       "94              1.602494                             0.045197   \n",
       "95              1.848915                             0.879763   \n",
       "104            10.032889                            17.392246   \n",
       "108             7.445625                             2.579493   \n",
       "112             1.505634                             0.996992   \n",
       "114             5.670781                             4.757523   \n",
       "117            14.697113                             0.573609   \n",
       "120           498.854478                           566.500631   \n",
       "128             1.107313                             0.188122   \n",
       "129            16.746106                             0.020656   \n",
       "133             0.768667                             1.551010   \n",
       "134             1.200767                             1.476182   \n",
       "142             0.889292                             0.728782   \n",
       "146             2.582958                             1.058572   \n",
       "155             2.460016                             2.048725   \n",
       "5550            0.000670                             0.001197   \n",
       "5560            0.000411                             0.000743   \n",
       "5566            0.000626                             0.002080   \n",
       "5568            0.000670                             0.001147   \n",
       "5569            0.001021                             0.001152   \n",
       "5584            0.000498                             0.002011   \n",
       "5594            0.000433                             0.003451   \n",
       "5595            0.000574                             0.000986   \n",
       "5598            0.000641                             0.001011   \n",
       "5599            0.000419                             0.000440   \n",
       "5601            0.000665                             0.000583   \n",
       "5608            0.000408                             0.000566   \n",
       "\n",
       "      train_per_1000_inst_mean_Accuracy  Accuracy__test_mean_min  \\\n",
       "2                              2.524716                 0.601449   \n",
       "3                              2.248931                 0.490545   \n",
       "7                              0.993208                 0.298795   \n",
       "13                             0.076460                 0.747838   \n",
       "16                             5.143934                 0.799234   \n",
       "18                             0.270177                 0.751864   \n",
       "21                             0.016105                 0.569102   \n",
       "22                             0.103336                 0.563360   \n",
       "29                             1.524957                 0.317188   \n",
       "30                           878.758593                 0.260474   \n",
       "32                             1.989965                 0.785637   \n",
       "49                             2.079466                 0.486111   \n",
       "50                             9.853124                 0.627928   \n",
       "54                             2.748751                 0.633333   \n",
       "55                             2.175462                 0.689000   \n",
       "64                             0.097635                 0.625287   \n",
       "65                             0.281881                 0.686187   \n",
       "82                             3.296120                 0.639540   \n",
       "84                             0.054797                 0.640622   \n",
       "92                             0.879336                 0.746014   \n",
       "94                             0.044693                 0.712198   \n",
       "95                             1.095877                 0.776705   \n",
       "104                           84.964077                 0.269524   \n",
       "108                            4.653516                 0.639000   \n",
       "112                            0.941021                 0.391000   \n",
       "114                           11.793725                 0.637350   \n",
       "117                            0.533045                 0.938691   \n",
       "120                          389.977908                 0.008754   \n",
       "128                            0.256140                 0.716878   \n",
       "129                            0.020422                 0.304311   \n",
       "133                            1.429437                 0.607199   \n",
       "134                            1.422597                 0.666379   \n",
       "142                            0.961540                 0.778561   \n",
       "146                            1.012131                 0.550470   \n",
       "155                            3.634975                 0.252941   \n",
       "5550                           0.001214                 0.601449   \n",
       "5560                           0.000822                 0.785637   \n",
       "5566                           0.002126                 0.627928   \n",
       "5568                           0.001215                 0.633333   \n",
       "5569                           0.001277                 0.689000   \n",
       "5584                           0.002118                 0.639540   \n",
       "5594                           0.003661                 0.269524   \n",
       "5595                           0.001195                 0.637350   \n",
       "5598                           0.001192                 0.607199   \n",
       "5599                           0.000496                 0.666379   \n",
       "5601                           0.000719                 0.778561   \n",
       "5608                           0.000603                 0.252941   \n",
       "\n",
       "      Accuracy__test_mean_max  ...  Log Loss__test_mean_max  \\\n",
       "2                    0.872464  ...                 1.682198   \n",
       "3                    0.796848  ...                 0.967133   \n",
       "7                    0.699889  ...                 5.715575   \n",
       "13                   0.946226  ...                 0.623800   \n",
       "16                   0.871090  ...                 1.027214   \n",
       "18                   0.857521  ...                 1.003083   \n",
       "21                   0.671467  ...                 1.075940   \n",
       "22                   0.704762  ...                 0.679891   \n",
       "29                   0.954493  ...                 6.514097   \n",
       "30                   0.800988  ...                 8.816495   \n",
       "32                   0.990348  ...                 2.533980   \n",
       "49                   0.957407  ...                 1.467915   \n",
       "50                   0.872297  ...                 1.503416   \n",
       "54                   0.884058  ...                 1.355583   \n",
       "55                   0.767000  ...                 1.489243   \n",
       "64                   0.938140  ...                 0.677321   \n",
       "65                   0.903127  ...                 1.016360   \n",
       "82                   0.843448  ...                 3.639566   \n",
       "84                   0.732004  ...                 0.639220   \n",
       "92                   0.816006  ...                 0.841618   \n",
       "94                   0.950958  ...                 2.230348   \n",
       "95                   0.862013  ...                 1.136808   \n",
       "104                  0.844286  ...                 5.534808   \n",
       "108                  0.864500  ...                 2.553028   \n",
       "112                  0.888500  ...                 6.847110   \n",
       "114                  1.000000  ...                 0.743279   \n",
       "117                  0.973306  ...                 0.306727   \n",
       "120                  0.846808  ...                20.778965   \n",
       "128                  0.908032  ...                 0.580233   \n",
       "129                  0.932046  ...                 3.277077   \n",
       "133                  0.744030  ...                 2.137502   \n",
       "134                  0.885328  ...                 1.460949   \n",
       "142                  0.955015  ...                 0.752412   \n",
       "146                  0.967398  ...                 9.403385   \n",
       "155                  0.847493  ...                12.908916   \n",
       "5550                 0.872464  ...                 1.682198   \n",
       "5560                 0.990348  ...                 2.533980   \n",
       "5566                 0.872297  ...                 1.503416   \n",
       "5568                 0.884058  ...                 1.355583   \n",
       "5569                 0.767000  ...                 1.489243   \n",
       "5584                 0.843448  ...                 3.639566   \n",
       "5594                 0.844286  ...                 5.534808   \n",
       "5595                 1.000000  ...                 0.743279   \n",
       "5598                 0.744030  ...                 2.137502   \n",
       "5599                 0.885328  ...                 1.460949   \n",
       "5601                 0.955015  ...                 0.752412   \n",
       "5608                 0.847493  ...                12.908916   \n",
       "\n",
       "      normalized_AUC__test_mean  normalized_AUC__test_std  \\\n",
       "2                      0.992585                  0.100835   \n",
       "3                      0.971437                  0.050844   \n",
       "7                      0.899894                  0.019957   \n",
       "13                     0.985543                  0.016432   \n",
       "16                     0.959466                  0.059792   \n",
       "18                     1.000000                  0.067645   \n",
       "21                     0.933155                  0.013691   \n",
       "22                     1.000000                  0.013941   \n",
       "29                     0.953854                  0.007902   \n",
       "30                     0.814579                  0.234645   \n",
       "32                     0.816782                  0.082053   \n",
       "49                     0.980417                  0.016809   \n",
       "50                     0.980441                  0.156481   \n",
       "54                     0.976911                  0.145686   \n",
       "55                     0.928016                  0.185722   \n",
       "64                     0.848551                  0.013093   \n",
       "65                     0.984999                  0.011247   \n",
       "82                     0.901218                  0.167460   \n",
       "84                     0.917294                  0.055919   \n",
       "92                     0.903119                  0.379115   \n",
       "94                     0.834268                  0.012087   \n",
       "95                     0.951763                  0.132173   \n",
       "104                    0.874895                  0.259187   \n",
       "108                    0.925951                  0.055302   \n",
       "112                    0.895176                  0.025367   \n",
       "114                    0.962354                  0.056971   \n",
       "117                    0.963238                  0.025208   \n",
       "120                    1.000000                  0.002826   \n",
       "128                    0.944094                  0.034245   \n",
       "129                    1.000000                  0.017530   \n",
       "133                    1.000000                  0.297807   \n",
       "134                    0.877756                  0.215603   \n",
       "142                    0.972811                  0.053618   \n",
       "146                    0.995352                  0.007776   \n",
       "155                    0.913797                  0.025148   \n",
       "5550                   0.936633                  0.132862   \n",
       "5560                   0.999588                  0.006998   \n",
       "5566                   0.903773                  0.126267   \n",
       "5568                   0.960585                  0.143565   \n",
       "5569                   0.899884                  0.130745   \n",
       "5584                   0.883443                  0.170938   \n",
       "5594                   0.921676                  0.176856   \n",
       "5595                   1.000000                  0.000000   \n",
       "5598                   0.635560                  0.144438   \n",
       "5599                   1.000000                  0.223149   \n",
       "5601                   0.926726                  0.075297   \n",
       "5608                   1.000000                  0.015812   \n",
       "\n",
       "      train_per_1000_inst_median_AUC  train_per_1000_inst_mean_AUC  \\\n",
       "2                           2.114068                      1.998640   \n",
       "3                           2.237972                      2.143907   \n",
       "7                           0.879405                      0.800541   \n",
       "13                          0.085092                      0.085040   \n",
       "16                          6.341648                     10.479124   \n",
       "18                          0.084726                      0.183536   \n",
       "21                          0.016039                      0.016105   \n",
       "22                          0.096953                      0.097484   \n",
       "29                          1.530976                      1.524957   \n",
       "30                         18.259061                   1098.535194   \n",
       "32                          1.892889                      1.934961   \n",
       "49                          2.134351                      2.979422   \n",
       "50                          2.827953                      4.881225   \n",
       "54                          1.597309                      1.721987   \n",
       "55                          2.361788                      3.148109   \n",
       "64                          0.097330                      0.097635   \n",
       "65                          0.222029                      0.268800   \n",
       "82                          0.906026                      2.166365   \n",
       "84                          0.027851                      0.044075   \n",
       "92                          0.716898                      0.936770   \n",
       "94                          0.172652                      0.148111   \n",
       "95                          1.274706                      1.433158   \n",
       "104                        31.704225                     98.856143   \n",
       "108                         1.683642                      1.702867   \n",
       "112                         0.711991                      0.852579   \n",
       "114                         6.674921                     11.681131   \n",
       "117                         0.573609                      0.572900   \n",
       "120                       566.558735                    526.507768   \n",
       "128                         0.248012                      0.284848   \n",
       "129                         1.463282                      1.466485   \n",
       "133                         0.866363                      1.206552   \n",
       "134                         1.595759                      1.976704   \n",
       "142                         0.390695                      0.619440   \n",
       "146                         0.976559                      1.049103   \n",
       "155                         2.036000                      3.317002   \n",
       "5550                        0.001197                      0.001214   \n",
       "5560                        0.000743                      0.000822   \n",
       "5566                        0.002080                      0.002126   \n",
       "5568                        0.001147                      0.001215   \n",
       "5569                        0.001152                      0.001277   \n",
       "5584                        0.002011                      0.002118   \n",
       "5594                        0.003451                      0.003661   \n",
       "5595                        0.000986                      0.001195   \n",
       "5598                        0.001011                      0.001192   \n",
       "5599                        0.000440                      0.000496   \n",
       "5601                        0.000583                      0.000719   \n",
       "5608                        0.000566                      0.000603   \n",
       "\n",
       "      AUC_rank_mean  AUC__test_mean  AUC__test_mean_min  AUC__test_mean_max  \\\n",
       "2               2.0        0.942324            0.759653            0.943688   \n",
       "3               5.0        0.865652            0.495182            0.876545   \n",
       "7               4.0        0.862134            0.500000            0.902419   \n",
       "13              4.0        0.984494            0.907805            0.985619   \n",
       "16              5.0        0.869733            0.679571            0.877767   \n",
       "18              1.0        0.906147            0.667600            0.906147   \n",
       "21              4.0        0.714689            0.573717            0.724788   \n",
       "22              1.0        0.776825            0.593685            0.776825   \n",
       "29              5.0        0.988423            0.790613            0.997992   \n",
       "30             17.0        0.877642            0.531318            0.956476   \n",
       "32             27.0        0.950065            0.730546            0.999306   \n",
       "49             15.0        0.994483            0.811468            0.998139   \n",
       "50              5.0        0.904803            0.562650            0.911629   \n",
       "54              2.0        0.938801            0.692580            0.944620   \n",
       "55              8.0        0.774905            0.507857            0.795619   \n",
       "64              5.0        0.936853            0.669873            0.984503   \n",
       "65              5.0        0.944335            0.574184            0.949972   \n",
       "82             15.0        0.886350            0.629070            0.914551   \n",
       "84              7.0        0.802189            0.682077            0.813019   \n",
       "92              5.0        0.868502            0.822619            0.873424   \n",
       "94              8.0        0.969604            0.836496            0.996047   \n",
       "95              6.0        0.814911            0.559898            0.827835   \n",
       "104            16.0        0.885764            0.506961            0.939931   \n",
       "108             7.0        0.982978            0.913231            0.988556   \n",
       "112            21.0        0.976364            0.868739            0.988967   \n",
       "114            24.0        0.981415            0.506336            1.000000   \n",
       "117             4.0        0.994576            0.960179            0.995889   \n",
       "120             1.0        0.997197            0.545225            0.997197   \n",
       "128             9.0        0.945472            0.678465            0.961283   \n",
       "129             1.0        0.933543            0.538271            0.933543   \n",
       "133             1.0        0.772984            0.478791            0.772984   \n",
       "134             8.0        0.930141            0.812509            0.946523   \n",
       "142             4.0        0.985848            0.834043            0.990091   \n",
       "146             7.0        0.992461            0.571933            0.994425   \n",
       "155            10.0        0.934469            0.570317            0.968821   \n",
       "5550            7.0        0.932026            0.759653            0.943688   \n",
       "5560            3.0        0.999195            0.730546            0.999306   \n",
       "5566           18.0        0.878048            0.562650            0.911629   \n",
       "5568            8.0        0.934686            0.692580            0.944620   \n",
       "5569           13.0        0.766810            0.507857            0.795619   \n",
       "5584           19.0        0.881276            0.629070            0.914551   \n",
       "5594            9.0        0.906019            0.506961            0.939931   \n",
       "5595            1.0        1.000000            0.506336            1.000000   \n",
       "5598           16.0        0.665769            0.478791            0.772984   \n",
       "5599            1.0        0.946523            0.812509            0.946523   \n",
       "5601           18.0        0.978656            0.834043            0.990091   \n",
       "5608            1.0        0.968821            0.570317            0.968821   \n",
       "\n",
       "      alg_type  \n",
       "2         gbdt  \n",
       "3         gbdt  \n",
       "7         gbdt  \n",
       "13        gbdt  \n",
       "16        gbdt  \n",
       "18        gbdt  \n",
       "21        gbdt  \n",
       "22        gbdt  \n",
       "29        gbdt  \n",
       "30        gbdt  \n",
       "32        gbdt  \n",
       "49        gbdt  \n",
       "50        gbdt  \n",
       "54        gbdt  \n",
       "55        gbdt  \n",
       "64        gbdt  \n",
       "65        gbdt  \n",
       "82        gbdt  \n",
       "84        gbdt  \n",
       "92        gbdt  \n",
       "94        gbdt  \n",
       "95        gbdt  \n",
       "104       gbdt  \n",
       "108       gbdt  \n",
       "112       gbdt  \n",
       "114       gbdt  \n",
       "117       gbdt  \n",
       "120       gbdt  \n",
       "128       gbdt  \n",
       "129       gbdt  \n",
       "133       gbdt  \n",
       "134       gbdt  \n",
       "142       gbdt  \n",
       "146       gbdt  \n",
       "155       gbdt  \n",
       "5550       pfn  \n",
       "5560       pfn  \n",
       "5566       pfn  \n",
       "5568       pfn  \n",
       "5569       pfn  \n",
       "5584       pfn  \n",
       "5594       pfn  \n",
       "5595       pfn  \n",
       "5598       pfn  \n",
       "5599       pfn  \n",
       "5601       pfn  \n",
       "5608       pfn  \n",
       "\n",
       "[47 rows x 38 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from analysis.table4 import DATASETS\n",
    "\n",
    "tuned_agg_df[tuned_agg_df[\"dataset_name\"].isin(DATASETS)][tuned_agg_df['alg_name'].isin(['CatBoost', 'TabFlex', 'TabPFN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4094171/1667309541.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  tuned_agg_df[tuned_agg_df[\"dataset_name\"].isin([\"openml__MiniBooNE__168335\"])][tuned_agg_df['alg_name'].isin(['CatBoost', 'TabFlex', 'TabPFN'])]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>Accuracy__test_median</th>\n",
       "      <th>Accuracy__test_mean</th>\n",
       "      <th>training_time_median</th>\n",
       "      <th>training_time_mean</th>\n",
       "      <th>train_per_1000_inst_median_Accuracy</th>\n",
       "      <th>train_per_1000_inst_mean_Accuracy</th>\n",
       "      <th>Accuracy__test_mean_min</th>\n",
       "      <th>Accuracy__test_mean_max</th>\n",
       "      <th>...</th>\n",
       "      <th>Log Loss__test_mean_max</th>\n",
       "      <th>normalized_AUC__test_mean</th>\n",
       "      <th>normalized_AUC__test_std</th>\n",
       "      <th>train_per_1000_inst_median_AUC</th>\n",
       "      <th>train_per_1000_inst_mean_AUC</th>\n",
       "      <th>AUC_rank_mean</th>\n",
       "      <th>AUC__test_mean</th>\n",
       "      <th>AUC__test_mean_min</th>\n",
       "      <th>AUC__test_mean_max</th>\n",
       "      <th>alg_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>openml__MiniBooNE__168335</td>\n",
       "      <td>0.942757</td>\n",
       "      <td>0.942813</td>\n",
       "      <td>8.844148</td>\n",
       "      <td>7.955771</td>\n",
       "      <td>0.084998</td>\n",
       "      <td>0.07646</td>\n",
       "      <td>0.747838</td>\n",
       "      <td>0.946226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6238</td>\n",
       "      <td>0.985543</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.085092</td>\n",
       "      <td>0.08504</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.984494</td>\n",
       "      <td>0.907805</td>\n",
       "      <td>0.985619</td>\n",
       "      <td>gbdt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    alg_name               dataset_name  Accuracy__test_median  \\\n",
       "13  CatBoost  openml__MiniBooNE__168335               0.942757   \n",
       "\n",
       "    Accuracy__test_mean  training_time_median  training_time_mean  \\\n",
       "13             0.942813              8.844148            7.955771   \n",
       "\n",
       "    train_per_1000_inst_median_Accuracy  train_per_1000_inst_mean_Accuracy  \\\n",
       "13                             0.084998                            0.07646   \n",
       "\n",
       "    Accuracy__test_mean_min  Accuracy__test_mean_max  ...  \\\n",
       "13                 0.747838                 0.946226  ...   \n",
       "\n",
       "    Log Loss__test_mean_max  normalized_AUC__test_mean  \\\n",
       "13                   0.6238                   0.985543   \n",
       "\n",
       "    normalized_AUC__test_std  train_per_1000_inst_median_AUC  \\\n",
       "13                  0.016432                        0.085092   \n",
       "\n",
       "    train_per_1000_inst_mean_AUC  AUC_rank_mean  AUC__test_mean  \\\n",
       "13                       0.08504            4.0        0.984494   \n",
       "\n",
       "    AUC__test_mean_min  AUC__test_mean_max  alg_type  \n",
       "13            0.907805            0.985619      gbdt  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_agg_df[tuned_agg_df[\"dataset_name\"].isin([\"openml__MiniBooNE__168335\"])][tuned_agg_df['alg_name'].isin(['CatBoost', 'TabFlex', 'TabPFN'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge in metafeatures and rewrite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# metafeatures_df = pd.read_csv(Path(\"metafeatures.csv\"))\n",
    "\n",
    "metafeatures_df.rename(columns={\"dataset_name\": \"dataset_fold_id\"}, inplace=True)\n",
    "\n",
    "metafeatures_df.loc[:, \"f__pymfe.general.total_num_instances\"] = metafeatures_df[\"f__pymfe.general.nr_inst\"] / 0.8\n",
    "metafeatures_df.loc[:, \"dataset_basename\"] = metafeatures_df[\"dataset_fold_id\"].str[:-len(\"__fold_1\")]\n",
    "agg_metafeatures = metafeatures_df.groupby(\"dataset_basename\").median(numeric_only=True)\n",
    "\n",
    "# remove histogram columns\n",
    "keep_cols = [c for c in agg_metafeatures.columns if \"hist\" not in c]\n",
    "\n",
    "# rename each column to be easier\n",
    "# new_col_names = {\n",
    "#     c: c.removeprefix(\"f__pymfe.\") for c in agg_metafeatures.columns\n",
    "# }\n",
    "# agg_metafeatures.rename(columns=new_col_names, inplace=True)\n",
    "\n",
    "round_attrs = [\n",
    "    \"f__pymfe.general.total_num_instances\",\n",
    "    \"f__pymfe.general.nr_attr\",\n",
    "    \"f__pymfe.general.nr_bin\",\n",
    "    \"f__pymfe.general.nr_cat\",\n",
    "    \"f__pymfe.general.nr_num\",\n",
    "    \"f__pymfe.general.nr_class\",\n",
    "]\n",
    "for attr in round_attrs:\n",
    "    agg_metafeatures.loc[:, attr] = agg_metafeatures[attr].round(0)\n",
    "\n",
    "agg_metafeatures.to_csv(output_folder / \"agg_metafeatures.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       183.000000\n",
       "mean      31353.909836\n",
       "std      105278.317705\n",
       "min          26.000000\n",
       "25%         490.500000\n",
       "50%        2027.500000\n",
       "75%       10747.000000\n",
       "max      820007.000000\n",
       "Name: f__pymfe.general.nr_inst, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_metafeatures[\"f__pymfe.general.nr_inst\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
